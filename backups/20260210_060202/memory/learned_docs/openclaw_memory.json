{
  "type": "markdown",
  "source": "/usr/lib/node_modules/openclaw/docs/concepts/memory.md",
  "title": "Memory",
  "description": "The default workspace layout uses two memory layers:\n\n- `memory/YYYY-MM-DD.md`\n  - Daily log (append-only).\n  - Read today + yesterday at session start.\n- `MEMORY.md` (optional)\n  - Curated long-term memory.\n  - **Only load in the main, private session** (never in group contexts).\n\nThese files live under the workspace (`agents.defaults.workspace`, default\n`~/.openclaw/workspace`). See [Agent workspace](/concepts/agent-workspace) for the full layout.",
  "sections": {
    "Memory files (Markdown)": "The default workspace layout uses two memory layers:\n\n- `memory/YYYY-MM-DD.md`\n  - Daily log (append-only).\n  - Read today + yesterday at session start.\n- `MEMORY.md` (optional)\n  - Curated long-term memory.\n  - **Only load in the main, private session** (never in group contexts).\n\nThese files live under the workspace (`agents.defaults.workspace`, default\n`~/.openclaw/workspace`). See [Agent workspace](/concepts/agent-workspace) for the full layout.",
    "When to write memory": "- Decisions, preferences, and durable facts go to `MEMORY.md`.\n- Day-to-day notes and running context go to `memory/YYYY-MM-DD.md`.\n- If someone says \"remember this,\" write it down (do not keep it in RAM).\n- This area is still evolving. It helps to remind the model to store memories; it will know what to do.\n- If you want something to stick, **ask the bot to write it** into memory.",
    "Automatic memory flush (pre-compaction ping)": "When a session is **close to auto-compaction**, OpenClaw triggers a **silent,\nagentic turn** that reminds the model to write durable memory **before** the\ncontext is compacted. The default prompts explicitly say the model _may reply_,\nbut usually `NO_REPLY` is the correct response so the user never sees this turn.\n\nThis is controlled by `agents.defaults.compaction.memoryFlush`:\n\n\nDetails:\n\n- **Soft threshold**: flush triggers when the session token estimate crosses\n  `contextWindow - reserveTokensFloor - softThresholdTokens`.\n- **Silent** by default: prompts include `NO_REPLY` so nothing is delivered.\n- **Two prompts**: a user prompt plus a system prompt append the reminder.\n- **One flush per compaction cycle** (tracked in `sessions.json`).\n- **Workspace must be writable**: if the session runs sandboxed with\n  `workspaceAccess: \"ro\"` or `\"none\"`, the flush is skipped.\n\nFor the full compaction lifecycle, see\n[Session management + compaction](/reference/session-management-compaction).",
    "Vector memory search": "OpenClaw can build a small vector index over `MEMORY.md` and `memory/*.md` so\nsemantic queries can find related notes even when wording differs.\n\nDefaults:\n\n- Enabled by default.\n- Watches memory files for changes (debounced).\n- Uses remote embeddings by default. If `memorySearch.provider` is not set, OpenClaw auto-selects:\n  1. `local` if a `memorySearch.local.modelPath` is configured and the file exists.\n  2. `openai` if an OpenAI key can be resolved.\n  3. `gemini` if a Gemini key can be resolved.\n  4. `voyage` if a Voyage key can be resolved.\n  5. Otherwise memory search stays disabled until configured.\n- Local mode uses node-llama-cpp and may require `pnpm approve-builds`.\n- Uses sqlite-vec (when available) to accelerate vector search inside SQLite.\n\nRemote embeddings **require** an API key for the embedding provider. OpenClaw\nresolves keys from auth profiles, `models.providers.*.apiKey`, or environment\nvariables. Codex OAuth only covers chat/completions and does **not** satisfy\nembeddings for memory search. For Gemini, use `GEMINI_API_KEY` or\n`models.providers.google.apiKey`. For Voyage, use `VOYAGE_API_KEY` or\n`models.providers.voyage.apiKey`. When using a custom OpenAI-compatible endpoint,\nset `memorySearch.remote.apiKey` (and optional `memorySearch.remote.headers`).",
    "QMD backend (experimental)": "Set `memory.backend = \"qmd\"` to swap the built-in SQLite indexer for\n[QMD](https://github.com/tobi/qmd): a local-first search sidecar that combines\nBM25 + vectors + reranking. Markdown stays the source of truth; OpenClaw shells\nout to QMD for retrieval. Key points:\n\n**Prereqs**\n\n- Disabled by default. Opt in per-config (`memory.backend = \"qmd\"`).\n- Install the QMD CLI separately (`bun install -g https://github.com/tobi/qmd` or grab\n  a release) and make sure the `qmd` binary is on the gateway’s `PATH`.\n- QMD needs an SQLite build that allows extensions (`brew install sqlite` on\n  macOS).\n- QMD runs fully locally via Bun + `node-llama-cpp` and auto-downloads GGUF\n  models from HuggingFace on first use (no separate Ollama daemon required).\n- The gateway runs QMD in a self-contained XDG home under\n  `~/.openclaw/agents/<agentId>/qmd/` by setting `XDG_CONFIG_HOME` and\n  `XDG_CACHE_HOME`.\n- OS support: macOS and Linux work out of the box once Bun + SQLite are\n  installed. Windows is best supported via WSL2.\n\n**How the sidecar runs**\n\n- The gateway writes a self-contained QMD home under\n  `~/.openclaw/agents/<agentId>/qmd/` (config + cache + sqlite DB).\n- Collections are created via `qmd collection add` from `memory.qmd.paths`\n  (plus default workspace memory files), then `qmd update` + `qmd embed` run\n  on boot and on a configurable interval (`memory.qmd.update.interval`,\n  default 5 m).\n- Boot refresh now runs in the background by default so chat startup is not\n  blocked; set `memory.qmd.update.waitForBootSync = true` to keep the previous\n  blocking behavior.\n- Searches run via `qmd query --json`. If QMD fails or the binary is missing,\n  OpenClaw automatically falls back to the builtin SQLite manager so memory tools\n  keep working.\n- OpenClaw does not expose QMD embed batch-size tuning today; batch behavior is\n  controlled by QMD itself.\n- **First search may be slow**: QMD may download local GGUF models (reranker/query\n  expansion) on the first `qmd query` run.\n  - OpenClaw sets `XDG_CONFIG_HOME`/`XDG_CACHE_HOME` automatically when it runs QMD.\n  - If you want to pre-download models manually (and warm the same index OpenClaw\n    uses), run a one-off query with the agent’s XDG dirs.\n\n    OpenClaw’s QMD state lives under your **state dir** (defaults to `~/.openclaw`).\n    You can point `qmd` at the exact same index by exporting the same XDG vars\n    OpenClaw uses:\n\n    ```bash\n    # Pick the same state dir OpenClaw uses\n    STATE_DIR=\"${OPENCLAW_STATE_DIR:-$HOME/.openclaw}\"\n    if [ -d \"$HOME/.moltbot\" ] && [ ! -d \"$HOME/.openclaw\" ] \\\n      && [ -z \"${OPENCLAW_STATE_DIR:-}\" ]; then\n      STATE_DIR=\"$HOME/.moltbot\"\n    fi\n\n    export XDG_CONFIG_HOME=\"$STATE_DIR/agents/main/qmd/xdg-config\"\n    export XDG_CACHE_HOME=\"$STATE_DIR/agents/main/qmd/xdg-cache\"\n\n    # (Optional) force an index refresh + embeddings\n    qmd update\n    qmd embed\n\n    # Warm up / trigger first-time model downloads\n    qmd query \"test\" -c memory-root --json >/dev/null 2>&1\n    ```\n\n**Config surface (`memory.qmd.*`)**\n\n- `command` (default `qmd`): override the executable path.\n- `includeDefaultMemory` (default `true`): auto-index `MEMORY.md` + `memory/**/*.md`.\n- `paths[]`: add extra directories/files (`path`, optional `pattern`, optional\n  stable `name`).\n- `sessions`: opt into session JSONL indexing (`enabled`, `retentionDays`,\n  `exportDir`).\n- `update`: controls refresh cadence and maintenance execution:\n  (`interval`, `debounceMs`, `onBoot`, `waitForBootSync`, `embedInterval`,\n  `commandTimeoutMs`, `updateTimeoutMs`, `embedTimeoutMs`).\n- `limits`: clamp recall payload (`maxResults`, `maxSnippetChars`,\n  `maxInjectedChars`, `timeoutMs`).\n- `scope`: same schema as [`session.sendPolicy`](/gateway/configuration#session).\n  Default is DM-only (`deny` all, `allow` direct chats); loosen it to surface QMD\n  hits in groups/channels.\n- When `scope` denies a search, OpenClaw logs a warning with the derived\n  `channel`/`chatType` so empty results are easier to debug.\n- Snippets sourced outside the workspace show up as\n  `qmd/<collection>/<relative-path>` in `memory_search` results; `memory_get`\n  understands that prefix and reads from the configured QMD collection root.\n- When `memory.qmd.sessions.enabled = true`, OpenClaw exports sanitized session\n  transcripts (User/Assistant turns) into a dedicated QMD collection under\n  `~/.openclaw/agents/<id>/qmd/sessions/`, so `memory_search` can recall recent\n  conversations without touching the builtin SQLite index.\n- `memory_search` snippets now include a `Source: <path#line>` footer when\n  `memory.citations` is `auto`/`on`; set `memory.citations = \"off\"` to keep\n  the path metadata internal (the agent still receives the path for\n  `memory_get`, but the snippet text omits the footer and the system prompt\n  warns the agent not to cite it).\n\n**Example**\n\n\n**Citations & fallback**\n\n- `memory.citations` applies regardless of backend (`auto`/`on`/`off`).\n- When `qmd` runs, we tag `status().backend = \"qmd\"` so diagnostics show which\n  engine served the results. If the QMD subprocess exits or JSON output can’t be\n  parsed, the search manager logs a warning and returns the builtin provider\n  (existing Markdown embeddings) until QMD recovers.",
    "Additional memory paths": "If you want to index Markdown files outside the default workspace layout, add\nexplicit paths:\n\n\nNotes:\n\n- Paths can be absolute or workspace-relative.\n- Directories are scanned recursively for `.md` files.\n- Only Markdown files are indexed.\n- Symlinks are ignored (files or directories).",
    "Gemini embeddings (native)": "Set the provider to `gemini` to use the Gemini embeddings API directly:\n\n\nNotes:\n\n- `remote.baseUrl` is optional (defaults to the Gemini API base URL).\n- `remote.headers` lets you add extra headers if needed.\n- Default model: `gemini-embedding-001`.\n\nIf you want to use a **custom OpenAI-compatible endpoint** (OpenRouter, vLLM, or a proxy),\nyou can use the `remote` configuration with the OpenAI provider:\n\n\nIf you don't want to set an API key, use `memorySearch.provider = \"local\"` or set\n`memorySearch.fallback = \"none\"`.\n\nFallbacks:\n\n- `memorySearch.fallback` can be `openai`, `gemini`, `local`, or `none`.\n- The fallback provider is only used when the primary embedding provider fails.\n\nBatch indexing (OpenAI + Gemini):\n\n- Enabled by default for OpenAI and Gemini embeddings. Set `agents.defaults.memorySearch.remote.batch.enabled = false` to disable.\n- Default behavior waits for batch completion; tune `remote.batch.wait`, `remote.batch.pollIntervalMs`, and `remote.batch.timeoutMinutes` if needed.\n- Set `remote.batch.concurrency` to control how many batch jobs we submit in parallel (default: 2).\n- Batch mode applies when `memorySearch.provider = \"openai\"` or `\"gemini\"` and uses the corresponding API key.\n- Gemini batch jobs use the async embeddings batch endpoint and require Gemini Batch API availability.\n\nWhy OpenAI batch is fast + cheap:\n\n- For large backfills, OpenAI is typically the fastest option we support because we can submit many embedding requests in a single batch job and let OpenAI process them asynchronously.\n- OpenAI offers discounted pricing for Batch API workloads, so large indexing runs are usually cheaper than sending the same requests synchronously.\n- See the OpenAI Batch API docs and pricing for details:\n  - [https://platform.openai.com/docs/api-reference/batch](https://platform.openai.com/docs/api-reference/batch)\n  - [https://platform.openai.com/pricing](https://platform.openai.com/pricing)\n\nConfig example:\n\n\nTools:\n\n- `memory_search` — returns snippets with file + line ranges.\n- `memory_get` — read memory file content by path.\n\nLocal mode:\n\n- Set `agents.defaults.memorySearch.provider = \"local\"`.\n- Provide `agents.defaults.memorySearch.local.modelPath` (GGUF or `hf:` URI).\n- Optional: set `agents.defaults.memorySearch.fallback = \"none\"` to avoid remote fallback.",
    "How the memory tools work": "- `memory_search` semantically searches Markdown chunks (~400 token target, 80-token overlap) from `MEMORY.md` + `memory/**/*.md`. It returns snippet text (capped ~700 chars), file path, line range, score, provider/model, and whether we fell back from local → remote embeddings. No full file payload is returned.\n- `memory_get` reads a specific memory Markdown file (workspace-relative), optionally from a starting line and for N lines. Paths outside `MEMORY.md` / `memory/` are rejected.\n- Both tools are enabled only when `memorySearch.enabled` resolves true for the agent.",
    "What gets indexed (and when)": "- File type: Markdown only (`MEMORY.md`, `memory/**/*.md`).\n- Index storage: per-agent SQLite at `~/.openclaw/memory/<agentId>.sqlite` (configurable via `agents.defaults.memorySearch.store.path`, supports `{agentId}` token).\n- Freshness: watcher on `MEMORY.md` + `memory/` marks the index dirty (debounce 1.5s). Sync is scheduled on session start, on search, or on an interval and runs asynchronously. Session transcripts use delta thresholds to trigger background sync.\n- Reindex triggers: the index stores the embedding **provider/model + endpoint fingerprint + chunking params**. If any of those change, OpenClaw automatically resets and reindexes the entire store.",
    "Hybrid search (BM25 + vector)": "When enabled, OpenClaw combines:\n\n- **Vector similarity** (semantic match, wording can differ)\n- **BM25 keyword relevance** (exact tokens like IDs, env vars, code symbols)\n\nIf full-text search is unavailable on your platform, OpenClaw falls back to vector-only search.",
    "Why hybrid?": "Vector search is great at “this means the same thing”:\n\n- “Mac Studio gateway host” vs “the machine running the gateway”\n- “debounce file updates” vs “avoid indexing on every write”\n\nBut it can be weak at exact, high-signal tokens:\n\n- IDs (`a828e60`, `b3b9895a…`)\n- code symbols (`memorySearch.query.hybrid`)\n- error strings (“sqlite-vec unavailable”)\n\nBM25 (full-text) is the opposite: strong at exact tokens, weaker at paraphrases.\nHybrid search is the pragmatic middle ground: **use both retrieval signals** so you get\ngood results for both “natural language” queries and “needle in a haystack” queries.",
    "How we merge results (the current design)": "Implementation sketch:\n\n1. Retrieve a candidate pool from both sides:\n\n- **Vector**: top `maxResults * candidateMultiplier` by cosine similarity.\n- **BM25**: top `maxResults * candidateMultiplier` by FTS5 BM25 rank (lower is better).\n\n2. Convert BM25 rank into a 0..1-ish score:\n\n- `textScore = 1 / (1 + max(0, bm25Rank))`\n\n3. Union candidates by chunk id and compute a weighted score:\n\n- `finalScore = vectorWeight * vectorScore + textWeight * textScore`\n\nNotes:\n\n- `vectorWeight` + `textWeight` is normalized to 1.0 in config resolution, so weights behave as percentages.\n- If embeddings are unavailable (or the provider returns a zero-vector), we still run BM25 and return keyword matches.\n- If FTS5 can’t be created, we keep vector-only search (no hard failure).\n\nThis isn’t “IR-theory perfect”, but it’s simple, fast, and tends to improve recall/precision on real notes.\nIf we want to get fancier later, common next steps are Reciprocal Rank Fusion (RRF) or score normalization\n(min/max or z-score) before mixing.\n\nConfig:",
    "Embedding cache": "OpenClaw can cache **chunk embeddings** in SQLite so reindexing and frequent updates (especially session transcripts) don't re-embed unchanged text.\n\nConfig:",
    "Session memory search (experimental)": "You can optionally index **session transcripts** and surface them via `memory_search`.\nThis is gated behind an experimental flag.\n\n\nNotes:\n\n- Session indexing is **opt-in** (off by default).\n- Session updates are debounced and **indexed asynchronously** once they cross delta thresholds (best-effort).\n- `memory_search` never blocks on indexing; results can be slightly stale until background sync finishes.\n- Results still include snippets only; `memory_get` remains limited to memory files.\n- Session indexing is isolated per agent (only that agent’s session logs are indexed).\n- Session logs live on disk (`~/.openclaw/agents/<agentId>/sessions/*.jsonl`). Any process/user with filesystem access can read them, so treat disk access as the trust boundary. For stricter isolation, run agents under separate OS users or hosts.\n\nDelta thresholds (defaults shown):",
    "SQLite vector acceleration (sqlite-vec)": "When the sqlite-vec extension is available, OpenClaw stores embeddings in a\nSQLite virtual table (`vec0`) and performs vector distance queries in the\ndatabase. This keeps search fast without loading every embedding into JS.\n\nConfiguration (optional):\n\n\nNotes:\n\n- `enabled` defaults to true; when disabled, search falls back to in-process\n  cosine similarity over stored embeddings.\n- If the sqlite-vec extension is missing or fails to load, OpenClaw logs the\n  error and continues with the JS fallback (no vector table).\n- `extensionPath` overrides the bundled sqlite-vec path (useful for custom builds\n  or non-standard install locations).",
    "Local embedding auto-download": "- Default local embedding model: `hf:ggml-org/embeddinggemma-300M-GGUF/embeddinggemma-300M-Q8_0.gguf` (~0.6 GB).\n- When `memorySearch.provider = \"local\"`, `node-llama-cpp` resolves `modelPath`; if the GGUF is missing it **auto-downloads** to the cache (or `local.modelCacheDir` if set), then loads it. Downloads resume on retry.\n- Native build requirement: run `pnpm approve-builds`, pick `node-llama-cpp`, then `pnpm rebuild node-llama-cpp`.\n- Fallback: if local setup fails and `memorySearch.fallback = \"openai\"`, we automatically switch to remote embeddings (`openai/text-embedding-3-small` unless overridden) and record the reason.",
    "Custom OpenAI-compatible endpoint example": "Notes:\n\n- `remote.*` takes precedence over `models.providers.openai.*`.\n- `remote.headers` merge with OpenAI headers; remote wins on key conflicts. Omit `remote.headers` to use the OpenAI defaults."
  },
  "commands": [],
  "code_blocks": [
    {
      "language": "json5",
      "content": "{\n  agents: {\n    defaults: {\n      compaction: {\n        reserveTokensFloor: 20000,\n        memoryFlush: {\n          enabled: true,\n          softThresholdTokens: 4000,\n          systemPrompt: \"Session nearing compaction. Store durable memories now.\",\n          prompt: \"Write any lasting notes to memory/YYYY-MM-DD.md; reply with NO_REPLY if nothing to store.\",\n        },\n      },\n    },\n  },\n}"
    },
    {
      "language": "json5",
      "content": "memory: {\n  backend: \"qmd\",\n  citations: \"auto\",\n  qmd: {\n    includeDefaultMemory: true,\n    update: { interval: \"5m\", debounceMs: 15000 },\n    limits: { maxResults: 6, timeoutMs: 4000 },\n    scope: {\n      default: \"deny\",\n      rules: [{ action: \"allow\", match: { chatType: \"direct\" } }]\n    },\n    paths: [\n      { name: \"docs\", path: \"~/notes\", pattern: \"**/*.md\" }\n    ]\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      extraPaths: [\"../team-docs\", \"/srv/shared-notes/overview.md\"]\n    }\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      provider: \"gemini\",\n      model: \"gemini-embedding-001\",\n      remote: {\n        apiKey: \"YOUR_GEMINI_API_KEY\"\n      }\n    }\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      provider: \"openai\",\n      model: \"text-embedding-3-small\",\n      remote: {\n        baseUrl: \"https://api.example.com/v1/\",\n        apiKey: \"YOUR_OPENAI_COMPAT_API_KEY\",\n        headers: { \"X-Custom-Header\": \"value\" }\n      }\n    }\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      provider: \"openai\",\n      model: \"text-embedding-3-small\",\n      fallback: \"openai\",\n      remote: {\n        batch: { enabled: true, concurrency: 2 }\n      },\n      sync: { watch: true }\n    }\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      query: {\n        hybrid: {\n          enabled: true,\n          vectorWeight: 0.7,\n          textWeight: 0.3,\n          candidateMultiplier: 4\n        }\n      }\n    }\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      cache: {\n        enabled: true,\n        maxEntries: 50000\n      }\n    }\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      experimental: { sessionMemory: true },\n      sources: [\"memory\", \"sessions\"]\n    }\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      sync: {\n        sessions: {\n          deltaBytes: 100000,   // ~100 KB\n          deltaMessages: 50     // JSONL lines\n        }\n      }\n    }\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      store: {\n        vector: {\n          enabled: true,\n          extensionPath: \"/path/to/sqlite-vec\"\n        }\n      }\n    }\n  }\n}"
    },
    {
      "language": "json5",
      "content": "agents: {\n  defaults: {\n    memorySearch: {\n      provider: \"openai\",\n      model: \"text-embedding-3-small\",\n      remote: {\n        baseUrl: \"https://api.example.com/v1/\",\n        apiKey: \"YOUR_REMOTE_API_KEY\",\n        headers: {\n          \"X-Organization\": \"org-id\",\n          \"X-Project\": \"project-id\"\n        }\n      }\n    }\n  }\n}"
    }
  ],
  "learned_from": "/usr/lib/node_modules/openclaw/docs/concepts/memory.md",
  "source_type": "file",
  "name": "openclaw_memory"
}